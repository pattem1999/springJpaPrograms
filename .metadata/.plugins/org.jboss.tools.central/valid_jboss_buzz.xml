<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><entry><title type="html">Smarter Decision Tables Generation through Data Types Constraints</title><link rel="alternate" href="https://blog.kie.org/2023/01/automatically-generating-decision-tables-in-dmn.html" /><author><name>Jozef Marko</name></author><id>https://blog.kie.org/2023/01/automatically-generating-decision-tables-in-dmn.html</id><updated>2023-01-17T19:57:51Z</updated><content type="html">The Decision Model and Notation (DMN) is an effective, yet standardized, tool for designing complex decisions, in special decision tables. Even though you, DMN developer, already knew about the existence of this capability, here’s something you didn’t know: there is a new DMN Decision Table column data type constraint enhancement in the DMN Editor. Let’s check all about this new feature (), available in the KIE Business Automation Tools starting version , that can facilitate and speed up the design and authoring of decision tables. In this article, you will understand when and how to use this new DMN feature part of the Business Automation Tools under the . As well described in the documentation, according to DMN specification, we can use data types to determine the structure of the data that we want to use within an associated decision table, column, or field in the boxed expression. Other than the default DMN data types (such as String, Number, Boolean) we can create custom data types to specify additional fields and constraints. &gt; TIP: SHOULD I USE CONSTRAINTS IN MY MODELS? &gt; &gt; We do not apply constraints in real world so often. We have infinite amount of &gt; colors, places, vendors, etc.. However as DMN developers, we restrict the &gt; domain usually. Here we will develop DMN traffic lights example where we will &gt; check just red and green color. Yes, we know that some countries may use also &gt; orange one and some other traffic lights may use totally different colors. &gt; However it is just a proof that as DMN developers we need constraints. Now, looking back to the way we used to work with decision tables, there was the need to manually synchronize every decision table column with the constraints of data types being used. Fortunately, from now on: the Decisions Editor is responsible for synchronization automatically during the first generation of the decision table. From this point on, we have a great feature at hand to help us out, but it is important to keep in mind that the editor does not synchronize all your following changes in Data Types and in the Existing Decision Table. In other words, the synchronization will help you out the first time you generate a decision table.  Now, let’s see how we can use these new decision table capabilities. REQUIREMENTS In order to try out this enhancement, you will need one of the environments from the table below: VS Code IDE v9.9+ with The online editor Now, let’s understand a bit more about the usage of constraints and how to set them up.  HOW TO DEFINE DATA TYPE CONSTRAINTS So we need data type constraints. In the DMN Editor, we have a separate Data Types tab that allows us to manage the data types.  You can find detailed information and how-to-use guides available not only in our other , but in the community as well. To get started, design a new data type tColor like below: If we look more precisely, it contains just two colors (red and green) that are allowed values of tColor data type. Now that we have defined our data type, we can proceed to use DMN to design the Traffic Lights decisions. The Traffic Lights decisions will be represented in what is called a Decision Requirement Diagram (DRD). DESIGNING THE DECISION TABLE In regards to the logic of the decision table we will author, we can consider as an input the color of the traffic light, which is either red or green:  When this decision is a red color, we can not walk.When there is a green color, we can walk.  Next, DMN allows us to define additional data types with a constraint – tMeaning. Let’s see how to do it below: So we have the data type tMeaning and two constraints that define which are the allowed values, in this case, “do not walk” and “walk”. Probably you already have an idea of how it is related to “red” and “green” colors. Moving forward, let’s look at the DRD.  The main diagram will contain just two nodes that can be configured as follows: The first node is an Input Data node. The Input Data node represents the color of the traffic light.We can set its type reference as tColor, according to its business meaning.Add a new Decision Node that will help us figure out which is the meaning of the received input traffic light color.Connect the Input Data and the Decision node.The Decision Node: represents the meaning. This is where our custom type tMeaning becomes useful! Now, we should set the decision type reference as the type tMeaning. * At this point, we have two connected nodes with types set and we miss the last, but not least, thing, and it is a decision table with column data type constraint. Now comes the fun part which we can see the new capability in action. AUTOMATICALLY DECISION TABLE GENERATION The DMN Editor is now able to pre-generate each column of the decision table based on the information requirements of each node. By defining the constraints in our custom data type, and configuring the table to this data type, we allowed the editor to automatically abstract which information could be part of the decision table, saving us some extra time! The only task is then to simply define two rows of the decision table.  See the demanded result below: In case you are still asking yourself, “So where is the promised enhancement?”, we want you to notice that until now, we have done a lot of manual configurations. However, if you check column header details, you will realize, the column data type constraint was automatically set. Simply click “Traffic Light Color” or “Human Meaning” cells and check the properties panel properly. The data type constraint proper configuration in the decision table’s header details is essential for an efficient decision table gap analysis. FEATURE EXTRA POINTS: GAP ANALYSIS Let’s say during the design phase, we forgot to handle one of the traffic light colors in our decision table. Thanks to column data type constraint, the tooling is now able to detect we are missing the validation of one data. This happens in the background, and as you are authoring your decision, the analysis of the model is running. Luckily, we can be assured that the tool has us covered, as it will display a message warning about exactly which light traffic color we forget to handle in our decision table. The full sample, including the complete diagram source code, is available as a gist . The post appeared first on .</content><dc:creator>Jozef Marko</dc:creator></entry><entry><title type="html">Using REST Services to upload and download files</title><link rel="alternate" href="http://www.mastertheboss.com/jboss-frameworks/resteasy/using-rest-services-to-manage-download-and-upload-of-files/" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/jboss-frameworks/resteasy/using-rest-services-to-manage-download-and-upload-of-files/</id><updated>2023-01-17T18:07:32Z</updated><content type="html">This REST Service tutorial is a quick guide for handling files upload and download using REST Services. We will create and test a Rest Service to upload and download files using JAX-RS API. Finally, we will show how to build a JUnit 5 Test to test files uploading using RESTEasy Client API. Uploading and Downloading ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title type="html">How to use Log4j2 in your WildFly applications</title><link rel="alternate" href="http://www.mastertheboss.com/jbossas/jboss-log/how-to-use-log4j2-in-your-wildfly-applications/" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/jbossas/jboss-log/how-to-use-log4j2-in-your-wildfly-applications/</id><updated>2023-01-17T15:16:53Z</updated><content type="html">Log4j2 is the latest major release of the popular Logging Framework. In this tutorial we will learn how to include Log4j2 configuration file and use it in your deployments running on WildFly. Overview of Log4j2 Log4j2 is a powerful logging library, developed by Apache, that provides advanced features such as: Asynchronous Logging: Log4j2 allows for ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title type="html">Deploy dashboards to OpenShift</title><link rel="alternate" href="https://blog.kie.org/2023/01/deploy-dashboards-to-openshift.html" /><author><name>Guilherme Caponetto</name></author><id>https://blog.kie.org/2023/01/deploy-dashboards-to-openshift.html</id><updated>2023-01-17T13:59:09Z</updated><content type="html">Starting from the KIE Tools release, the enables users to easily deploy their dashboards to OpenShift. Let’s check out how to set up and use this new feature in this post! Photo by on  INTRODUCTION makes it easy for users to create rich dashboards to accommodate their data in meaningful visualizations. Just like for serverless workflow files, the capability to deploy dashboards to OpenShift has also been added to . It allows users not only to design their dashboards but also to make them available to be shared with others through their OpenShift instance. The deploy button will be available on the toolbar once a dashboard is open in the editor. Users can either deploy only the dashboard they are currently visualizing or the entire workspace, which may have more dashboards and related data files to be deployed. Behind the scenes, the files are placed along with a that includes a web application and the viewer. On the OpenShift side, all necessary resources are created on top of Knative Serving and a builder is triggered to prepare the deployment. The deployment will scale up and be ready to be accessed once the building process is completed. After one minute of inactivity, the deployment will scale down to zero pods. In case of the deployment is accessed again, it will automatically scale up. This is Knative making sure you save resources of your OpenShift instance. SET UP CONNECTIONS There are two mandatory configurations that must be done in order to use the deploy operation: (1) run the KIE Sandbox Extended Services; (2) set up the OpenShift instance information. KIE SANDBOX EXTENDED SERVICES It is mandatory to run the KIE Sandbox Extended Services since it bridges the requests between the and OpenShift. If the KIE Sandbox Extended Services is not running, go to and click on the Settings button ️located in the top right corner of the page (⚙). Access the KIE Sandbox Extended Services tab and click on "Click to setup". Then follow the instructions for your Operating system to get the KIE Sandbox Extended Services up and running. KIE Sandbox Extended Services tab – not connected KIE Sandbox Extended Services tab — connected OPENSHIFT INSTANCE INFORMATION If the OpenShift instance information is not configured yet, go to and click on the Settings button ️located in the top right corner of the page (⚙). Access the OpenShift tab and fill in the fields with your OpenShift instance information (namespace/project, host, and token). Note: The personal access token from OpenShift expires every 24 hours. Note: You can use the , which gives you 30 days of free access to a shared OpenShift and Kubernetes cluster. You will only need your phone to confirm the activation of your instance. OpenShift tab — connected DEPLOY THE DASHBOARD Go to and open a dashboard. The sample will be used as an example for this post. Once the dashboard is ready to be deployed, click on "Try on OpenShift" located in the toolbar and "Deploy". A popup will be open for you to confirm this operation. Confirmation popup for deploying dashboards If you simply confirm, only the dashboard you are currently visualizing will be deployed. If you want to deploy the entire workspace, check the "Deploy workspace" box. After the confirmation, all resources will be created in your OpenShift instance and a builder will be started. You can follow the progress of the deployment operation either on or in your OpenShift instance. Once the building process is completed, the deployment will be scaled up and ready to be shared. Now, let’s take a look at this feature in action. And that’s all for today. Thanks for reading! 😃 The post appeared first on .</content><dc:creator>Guilherme Caponetto</dc:creator></entry><entry><title type="html">How to configure SLF4J in WildFly applications</title><link rel="alternate" href="http://www.mastertheboss.com/jbossas/jboss-log/how-to-configure-slf4j-in-wildfly-applications/" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/jbossas/jboss-log/how-to-configure-slf4j-in-wildfly-applications/</id><updated>2023-01-17T13:15:59Z</updated><content type="html">In this tutorial, we will discuss how to use Simple Logging Facade for Java (SLF4J) with Wildfly application server. SLF4J is a logging facade that provides a unified interface for various logging frameworks, such as log4j, java.util.logging, and Logback. It allows for the decoupling of the application code from the underlying logging framework, making it ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title type="html">Getting Started with Simple Logging Facade (SLF4J)</title><link rel="alternate" href="http://www.mastertheboss.com/jbossas/jboss-log/getting-started-with-simple-logging-facade-slf4j/" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/jbossas/jboss-log/getting-started-with-simple-logging-facade-slf4j/</id><updated>2023-01-17T12:47:53Z</updated><content type="html">Simple Logging Facade for Java (SLF4J) is a logging facade that provides a unified interface for various logging frameworks, such as log4j, java.util.logging, and Logback. It allows for the decoupling of the application code from the underlying logging framework, making it easier to change the logging framework without modifying the application code. Here is a ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title type="html">Kogito 1.32.0 released!</title><link rel="alternate" href="https://blog.kie.org/2023/01/kogito-1-32-0-released.html" /><author><name>Cristiano Nicolai</name></author><id>https://blog.kie.org/2023/01/kogito-1-32-0-released.html</id><updated>2023-01-17T12:38:34Z</updated><content type="html">We are glad to announce that the Kogito 1.32.0 release is now available! This goes hand in hand with , release. From a feature point of view, we have included a series of new features and bug fixes, including: * Quarkus 2.15 integration * Integration with Flyway for PostgreSQL data migration for process runtime persistence in the following addons: kogito-addons-quarkus-persistence-postgresql and kogito-addons-springboot-persistence-postgresql. * Data Index Integration with Flyway for PostgreSQL persistence.  * Sleep parameter can be used in action to define the time periods that the workflow execution should sleep before/after executing the function. It needs to be used as mentioned in the Specification . BREAKING CHANGES * Removed kogito.persistence.auto.ddl property for PostgreSQL persistence addons ( kogito-addons-quarkus-persistence-postgresql and kogito-addons-springboot-persistence-postgresql ). Please refer to using Flyway integration in order to handle database creation. For more details head to the complete . All artifacts are available now: * Kogito runtime artifacts are available on Maven Central. * Kogito examples can be found . * Kogito images are available on . * Kogito operator is available in the in OpenShift and Kubernetes. * Kogito tooling 0.26.0 artifacts are available at the . A detailed changelog for 1.32.0 can be found in . New to Kogito? Check out our website . Click the "Get Started" button. The post appeared first on .</content><dc:creator>Cristiano Nicolai</dc:creator></entry><entry><title type="html">Configure Hibernate Connection Pool</title><link rel="alternate" href="http://www.mastertheboss.com/hibernate-jpa/hibernate-configuration/configure-a-connection-pool-with-hibernate/" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/hibernate-jpa/hibernate-configuration/configure-a-connection-pool-with-hibernate/</id><updated>2023-01-17T08:15:43Z</updated><content type="html">This article discusses how to configure a JDBC Connection Pool in Hibernate applications covering both Hibernate managed applications and Hibernate Native applications. A JDBC connection pool is a set of connections to a database that are created and maintained by an application, rather than creating a new connection each time it needs to interact with ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title>How to deploy .NET apps as systemd services using containers</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/01/17/how-deploy-net-apps-systemd-services-using-containers" /><author><name>Tom Deseyn</name></author><id>b9809022-befa-4215-8407-f6bce7e8ca5a</id><updated>2023-01-17T07:00:00Z</updated><published>2023-01-17T07:00:00Z</published><summary type="html">&lt;p&gt;Under &lt;a href="https://developers.redhat.com/topics/linux"&gt;Linux&lt;/a&gt;, services should not be simply launched but should be managed by the &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/system_administrators_guide/chap-managing_services_with_systemd"&gt;systemd daemon&lt;/a&gt; for efficiency and better control. To deploy a &lt;a href="https://developers.redhat.com/topics/dotnet"&gt;.NET&lt;/a&gt; application as a systemd service, you need to determine how to distribute the application, how to manage upgrades, and how to handle its native dependencies. &lt;a href="https://developers.redhat.com/topics/containers"&gt;Containers&lt;/a&gt; solve these problems because container images are distributed through registries, images are versioned using tags, allowing for easy upgrades, and an image includes all the dependencies.&lt;/p&gt; &lt;p&gt;When deploying to Kubernetes, packing up your .NET application as a container for systemd gives it the benefits of being self-contained and universally deployable.&lt;/p&gt; &lt;h2&gt;Creating an application&lt;/h2&gt; &lt;p&gt;As an example for this article, we'll use an application based on the &lt;code&gt;Worker&lt;/code&gt; template. The &lt;code&gt;Worker&lt;/code&gt; template is a console application based on &lt;code&gt;Microsoft.Extension.Hosting&lt;/code&gt; and provides the configuration and dependency injection services you know from ASP.NET Core. &lt;code&gt;Worker&lt;/code&gt; is meant for long-running applications that aren't web applications. You can learn more about the &lt;code&gt;Worker&lt;/code&gt; template in &lt;a href="https://learn.microsoft.com/en-us/dotnet/core/extensions/workers"&gt;Worker Services&lt;/a&gt;. What we do in this article also applies to ASP.NET Core.&lt;/p&gt; &lt;p&gt;Create the application for this article as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ dotnet new worker -o worker $ cd worker&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If you open up the application folder, you'll find the &lt;code&gt;Worker.cs&lt;/code&gt; file, which implements a simple long-running job that prints a message to the console once a second.&lt;/p&gt; &lt;p&gt;Execute &lt;code&gt;dotnet run&lt;/code&gt; to see the application in action. When you've seen a couple of messages, press Ctrl-C to terminate the application.&lt;/p&gt; &lt;p&gt;.NET Core 3.0 added support for systemd services. Specifically, the application host (from &lt;code&gt;Microsoft.Extension.Hosting&lt;/code&gt;) can notify systemd upon startup. For a web application, notifying systems indicates that the application is ready to serve requests. In our example, the notification means that all the hosted services, such as the &lt;code&gt;Worker&lt;/code&gt; class, have started for our worker.&lt;/p&gt; &lt;p&gt;If you want a &lt;code&gt;BackgroundService&lt;/code&gt; such as &lt;code&gt;Worker.cs&lt;/code&gt; to do some asynchronous initialization that affects readiness, you can allow time for that activity by overriding the base &lt;code&gt;StartAsync&lt;/code&gt; method, as shown in the following code:&lt;/p&gt; &lt;pre&gt; &lt;code class="cpp"&gt;public async override Task StartAsync(CancellationToken cancellationToken) { // Perform startup. System.Console.WriteLine("Starting"); await Task.Delay(500, cancellationToken); System.Console.WriteLine("Started"); // Call base method. await base.StartAsync(cancellationToken); }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In .NET 7, systemd support was extended to include .NET running in a container managed by systemd. If you are using an earlier version of .NET, systemd can run your container, but systemd won't know when your application is fully ready.&lt;/p&gt; &lt;p&gt;To enable systemd integration, add the &lt;code&gt;Microsoft.Extensions.Hosting.Systemd&lt;/code&gt; package:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ dotnet add package Microsoft.Extensions.Hosting.Systemd&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Next, call &lt;code&gt;UseSystemd&lt;/code&gt; on your application's &lt;code&gt;IHostBuilder&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="cpp"&gt;IHost host = Host.CreateDefaultBuilder(args) .ConfigureServices(services =&gt; { services.AddHostedService&lt;Worker&gt;(); }) + .UseSystemd() .Build();&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You can add this code to any .NET application to make it runnable with systemd. If the application doesn't execute from systemd, the &lt;code&gt;UseSystemd&lt;/code&gt; call is a no-op.&lt;/p&gt; &lt;h2&gt;Creating a container image&lt;/h2&gt; &lt;p&gt;Now we're going to create a container image. There are a number of ways you can do this. You can hand-craft a &lt;code&gt;Containerfile&lt;/code&gt; (Dockerfile) based on Microsoft's &lt;a href="https://learn.microsoft.com/en-us/dotnet/core/docker/build-container?tabs=linux"&gt;Containerize a .NET app&lt;/a&gt; tutorial. You can use the new SDK feature that builds an image for you: as described in &lt;a href="https://devblogs.microsoft.com/dotnet/announcing-builtin-container-support-for-the-dotnet-sdk/"&gt;Announcing built-in container support for the .NET SDK&lt;/a&gt;. And a third option is to use the &lt;code&gt;dotnet build-image&lt;/code&gt; global tool that I introduced in the article &lt;a href="https://developers.redhat.com/articles/2022/08/01/containerize-net-applications-without-writing-dockerfiles"&gt;Containerize .NET applications without writing Dockerfiles&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Feel free to pick the option you prefer. I'm going to use the third option.&lt;/p&gt; &lt;p&gt;Install the global tool:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ dotnet tool install -g dotnet-build-image&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;And now, create the image:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ dotnet build-image -t awesome-worker-app:latest -b ubi&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;We're passing two arguments. &lt;code&gt;awesome-worker-app:latest&lt;/code&gt; is the name of the image. &lt;code&gt;ubi&lt;/code&gt; indicates that creation should be based on Red Hat's UBI images. You can pick another base, such as &lt;code&gt;alpine&lt;/code&gt; or &lt;code&gt;jammy-chiseled&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;For this tutorial, we'll deploy the systemd unit on the local system. To deploy it on several systems, you can set the tag to match the image registry (for example: &lt;code&gt;-t quay.io/tmds/awesome-worker-app:latest&lt;/code&gt;), and add the &lt;code&gt;--push&lt;/code&gt; argument to push the image to the registry after it was built.&lt;/p&gt; &lt;p&gt;The following command runs the image locally:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ podman run –rm awesome-worker-app:latest&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;As this executes, you'll see the messages from the containerized .NET application, similar to when executing &lt;code&gt;dotnet run&lt;/code&gt;. Terminate the container by pressing Ctrl+C.&lt;/p&gt; &lt;h2&gt;Creating a systemd unit&lt;/h2&gt; &lt;p&gt;We'll use Podman to generate a systemd unit file for us.&lt;/p&gt; &lt;p&gt;Start the container by running:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ podman run --name localhost/awesome-worker-app --rm --label "io.containers.autoupdate=local" awesome-worker-app:latest&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The command is similar to the one we used earlier. We're using the fully qualified name of the image (by adding the &lt;code&gt;localhost/&lt;/code&gt; prefix), and we've added a label (&lt;code&gt;io.containers.autoupdate&lt;/code&gt;), which we'll cover in a moment. If your image is deployed to a remote registry, set the label value to &lt;code&gt;registry&lt;/code&gt; instead of &lt;code&gt;local&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;If your application uses .NET 7, which supports systemd containers, you can add &lt;code&gt;--sdnotify=container&lt;/code&gt; to use that support.&lt;/p&gt; &lt;p&gt;In another terminal, run:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ podman generate systemd --new -n -f --start-timeout 600 awesome-worker-app&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This will write out a file named &lt;code&gt;container-awesome-worker-app.service&lt;/code&gt; with the following content:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;# container-awesome-worker-app.service # autogenerated by Podman 4.2.0 [Unit] Description=Podman container-awesome-worker-app.service Documentation=man:podman-generate-systemd(1) Wants=network-online.target After=network-online.target RequiresMountsFor=%t/containers [Service] Environment=PODMAN_SYSTEMD_UNIT=%n Restart=on-failure TimeoutStartSec=600 TimeoutStopSec=70 ExecStartPre=/bin/rm -f %t/%n.ctr-id ExecStart=/usr/bin/podman run \ --cidfile=%t/%n.ctr-id \ --cgroups=no-conmon \ --rm \ --sdnotify=conmon \ -d \ --replace \ --name awesome-worker-app \ --label io.containers.autoupdate=local localhost/awesome-worker-app:latest ExecStop=/usr/bin/podman stop --ignore --cidfile=%t/%n.ctr-id ExecStopPost=/usr/bin/podman rm -f --ignore --cidfile=%t/%n.ctr-id Type=notify NotifyAccess=all [Install] WantedBy=default.target&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This systemd file isn't specific to your system. It can be used to deploy the image anywhere.&lt;/p&gt; &lt;p&gt;We've provided a large &lt;code&gt;--start-timeout&lt;/code&gt; value of 10 minutes to allow the system time to pull the image (in case the image is hosted on a remote registry).&lt;/p&gt; &lt;p&gt;The label we added through the &lt;code&gt;--label&lt;/code&gt; argument shows up in the &lt;code&gt;ExecStart&lt;/code&gt; command. This label is detected by a &lt;a href="https://docs.podman.io/en/latest/markdown/podman-auto-update.1.html"&gt;podman auto-update&lt;/a&gt; command, which monitors all containers with this label. If a new image is available, the command pulls it and restarts the systemd unit.&lt;/p&gt; &lt;p&gt;Podman ships with a timer that triggers the update daily at midnight. If the new image fails to deploy, Podman even rolls back the container to the previous version. Failure detection works best if you use the &lt;code&gt;--sdnotify=container&lt;/code&gt; option (with .NET 7).&lt;/p&gt; &lt;p&gt;Now, if you host the image in an image registry solely using this unit file, you can deploy the .NET application to a large number of systems and even have it update automatically.&lt;/p&gt; &lt;h2&gt;Deploying the service&lt;/h2&gt; &lt;p&gt;You can deploy this unit with the system units at &lt;code&gt;/usr/systemd/system&lt;/code&gt;. We'll deploy it as part of our user's systemd services:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$mkdir -p ~/.config/systemd/user $ cp container-awesome-worker-app.service ~/.config/systemd/user/awesome-worker-app.service $ systemctl --user daemon-reload $ systemctl --user start awesome-worker-app&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;That's it—your service is up and running.&lt;/p&gt; &lt;p&gt;You can get the logs of the service by running the following:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ journalctl --user -f -u awesome-worker-app&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If you'd like this service to start at boot, you have to enable lingering for your user and enable the service:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$sudo loginctl enable-linger $(id -u) $ systemctl -- user start awesome-worker-app&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Triggering an update&lt;/h2&gt; &lt;p&gt;To finish the example, let's see auto-update in action.&lt;/p&gt; &lt;p&gt;First, change the message that is printed in &lt;code&gt;Worker.cs&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="cpp"&gt; { while (!stoppingToken.IsCancellationRequested) { - _logger.LogInformation("Worker running at: {time}", DateTimeOffset.Now); + _logger.LogInformation("Updated worker running at: {time}", DateTimeOffset.Now); await Task.Delay(1000, stoppingToken); } }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Then rebuild the image:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ dotnet build-image -t awesome-worker-app:latest -b ubi&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The new image is now available. A system that has the auto-update timer enabled will start using it at midnight. But you don't have to wait for that: to update the image the right way, just invoke &lt;code&gt;podman auto-update:&lt;/code&gt;&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ podman auto-update UNIT CONTAINER IMAGE POLICY UPDATED awesome-worker-app.service 342942f00932 (awesome-worker-app) awesome-worker-app:latest local true&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The command updates the service, and you can see the change in the journal:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$journalctl --user -f -u awesome-worker-app&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Running as a systemd service is straightforward&lt;/h2&gt; &lt;p&gt;This article has covered how to deploy .NET applications as systemd services using containers. Containers offer a convenient way to distribute, deploy, and update your application. You've also learned that applications using .NET 7 support notifying systemd about their readiness when running in containers.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/01/17/how-deploy-net-apps-systemd-services-using-containers" title="How to deploy .NET apps as systemd services using containers"&gt;How to deploy .NET apps as systemd services using containers&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Tom Deseyn</dc:creator><dc:date>2023-01-17T07:00:00Z</dc:date></entry><entry><title type="html">How to validate Database connections in JBoss / WildFly</title><link rel="alternate" href="http://www.mastertheboss.com/jbossas/jboss-datasource/how-to-automatically-reconnect-to-the-database-in-wildfly/" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/jbossas/jboss-datasource/how-to-automatically-reconnect-to-the-database-in-wildfly/</id><updated>2023-01-16T11:59:30Z</updated><content type="html">This tutorial discusses how to validate database connections using in WildFly so that you can manage reconnection to the database in case of temporary failures. Connection Validation in a nutshell A datasource connection validation helps to ensure that the connections to the database are still valid. The strategy behind connection vaidation is to periodically test ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry></feed>
